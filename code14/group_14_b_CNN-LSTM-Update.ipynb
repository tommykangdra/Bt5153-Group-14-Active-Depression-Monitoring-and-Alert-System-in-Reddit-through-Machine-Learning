{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"reddit_train.csv\",index_col=0,usecols=[0,4,6])\n",
    "test = pd.read_csv(\"reddit_test.csv\",index_col=0,usecols=[0,4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yidi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "\n",
    "try:\n",
    "    nltk.download('stopwords')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos(word):\n",
    "    # Part of Speech constants\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    pos= nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "    # Adjective tags -'JJ', 'JJR', 'JJS'\n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags -'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags -'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v':\n",
    "        return 'v'\n",
    "    # Noun tags -'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma_words = [] \n",
    "    wl= WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos= find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos))\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2Vbuilt\n",
      "Shape of label tensor: (254, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"&#xD;\", \" \").replace(\"&#xA;\", \" \").replace(\"&amp;\",'& ')\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]# and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"[^A-Za-z]\", \" \", text)\n",
    "    \n",
    "    text = words_lemmatizer(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def split_sent(x):\n",
    "    x= nltk.sent_tokenize(x)\n",
    "    return x\n",
    "\n",
    "def get_clean_tokens(sentence):\n",
    "    \n",
    "    cleaned = clean_text(sentence)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(cleaned)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# apply the above function to df['text']\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "def tokenize_sentences(df):\n",
    "    #Avoid directly modify original dataframe\n",
    "    df_temp = copy.deepcopy(df)\n",
    "   \n",
    "    df_temp['sentences'] = df_temp['Text'].apply(split_sent)\n",
    "    \n",
    "    df_temp['tokenized_sentences'] = list(map(lambda sentences: list(map(get_clean_tokens, sentences)), \n",
    "                                              df_temp.sentences))\n",
    "    #Remove all list == []\n",
    "    df_temp['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)),\n",
    "                                         df_temp.tokenized_sentences)) \n",
    "    \n",
    "    return df_temp[['Outcome','tokenized_sentences']]\n",
    "    \n",
    "train_data = tokenize_sentences(train)\n",
    "\n",
    "\n",
    "\n",
    "#Train word2vec model\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "train_sentences = [sentence for sentences in train_data.tokenized_sentences for sentence in sentences]\n",
    "\n",
    "W2Vmodel = Word2Vec(sentences=train_sentences, vector_size=50,sg=1, hs=0, workers=4, min_count=3, window=8,\n",
    "                    sample=1e-3, negative=5)\n",
    "\n",
    "print('W2Vbuilt')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train['Text'], train['Outcome'], test_size=0.2, stratify = train['Outcome'],random_state=3)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Text']= deepcopy(X_train)\n",
    "df['Outcome'] = deepcopy(y_train)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "df['Outcome'].value_counts()\n",
    "\n",
    "val= pd.DataFrame()\n",
    "val['Text']= deepcopy(X_val)\n",
    "val['Outcome'] = deepcopy(y_val)\n",
    "\n",
    "\n",
    "df_resample_1 = deepcopy(df[df['Outcome']==1]).sample(380, replace=True)\n",
    "df_resample_2 = deepcopy(df[df['Outcome']==2]).sample(500, replace=True)\n",
    "\n",
    "df_resample = pd.concat([df_resample_1, df_resample_2,df],  ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "df_resample['Text'] = df_resample['Text'].apply(clean_text)\n",
    "\n",
    "val['Text'] = val['Text'].apply(clean_text)\n",
    "test['Text'] = test['Text'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "len_95 = 200\n",
    "\n",
    "\n",
    "#Set a number which is larger than vocab to keep all useful information\n",
    "NUM_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "tokenizer.fit_on_texts(df_resample['Text'])\n",
    "X_train_sequences = tokenizer.texts_to_sequences(df_resample['Text'])\n",
    "X_eva_sequences = tokenizer.texts_to_sequences(val['Text'])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#Make input to be same length\n",
    "X_train = pad_sequences(X_train_sequences, maxlen=int(len_95))\n",
    "X_val = pad_sequences(X_eva_sequences, maxlen=int(len_95))\n",
    "X_test = pad_sequences(X_test_sequences, maxlen=int(len_95))\n",
    "\n",
    "y_train_dummy = pd.get_dummies(df_resample['Outcome']).values\n",
    "y_test_dummy = pd.get_dummies(val['Outcome']).values\n",
    "\n",
    "print('Shape of label tensor:', y_test_dummy.shape)\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "#Loading previous trained word2vec model\n",
    "word_vectors = W2Vmodel.wv\n",
    "EMBEDDING_DIM=50 #The same as word2vec features\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "#Give embedding matrix value according to word2vec model\n",
    "for word, i in word_index.items():\n",
    "    if i < vocabulary_size:     \n",
    "        try:\n",
    "            embedding_matrix[i] = word_vectors[word]\n",
    "        except KeyError:\n",
    "            #Ignore words not exist in train\n",
    "            embedding_matrix[i] = np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "    else: continue\n",
    "del(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import LSTM \n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D,GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten, Reshape,MaxPooling1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from math import sqrt \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 32 40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02208, saving model to weights_best_5153projet.hdfs\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02208\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.02208\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.02208\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.02208\n",
      "[[136  19   4]\n",
      " [ 44  16   4]\n",
      " [ 19   4   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.91      0.76       199\n",
      "           1       0.48      0.19      0.27        80\n",
      "           2       0.33      0.08      0.12        39\n",
      "\n",
      "    accuracy                           0.63       318\n",
      "   macro avg       0.49      0.39      0.38       318\n",
      "weighted avg       0.57      0.63      0.56       318\n",
      "\n",
      "[[181  13   5]\n",
      " [ 64  15   1]\n",
      " [ 33   3   3]]\n",
      "0.01 32 40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.06845, saving model to weights_best_5153projet.hdfs\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.06845\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.06845\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.06845\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.06845\n",
      "[[118  34   7]\n",
      " [ 35  25   4]\n",
      " [ 16   9   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.85      0.76       199\n",
      "           1       0.50      0.30      0.37        80\n",
      "           2       0.35      0.23      0.28        39\n",
      "\n",
      "    accuracy                           0.64       318\n",
      "   macro avg       0.51      0.46      0.47       318\n",
      "weighted avg       0.60      0.64      0.61       318\n",
      "\n",
      "[[169  20  10]\n",
      " [ 49  24   7]\n",
      " [ 26   4   9]]\n",
      "0.01 32 40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02124, saving model to weights_best_5153projet.hdfs\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02124\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.02124\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.02124\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.02124\n",
      "[[127  27   5]\n",
      " [ 30  29   5]\n",
      " [ 14  10   7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78       199\n",
      "           1       0.57      0.46      0.51        80\n",
      "           2       0.39      0.18      0.25        39\n",
      "\n",
      "    accuracy                           0.67       318\n",
      "   macro avg       0.56      0.50      0.51       318\n",
      "weighted avg       0.64      0.67      0.65       318\n",
      "\n",
      "[[170  20   9]\n",
      " [ 41  37   2]\n",
      " [ 24   8   7]]\n",
      "0.01 32 40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90751, saving model to weights_best_5153projet.hdfs\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.90751\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.90751\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.90751\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.90751\n",
      "[[136  17   6]\n",
      " [ 37  16  11]\n",
      " [ 17   8   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.90      0.79       199\n",
      "           1       0.48      0.30      0.37        80\n",
      "           2       0.33      0.10      0.16        39\n",
      "\n",
      "    accuracy                           0.65       318\n",
      "   macro avg       0.51      0.44      0.44       318\n",
      "weighted avg       0.60      0.65      0.61       318\n",
      "\n",
      "[[180  16   3]\n",
      " [ 51  24   5]\n",
      " [ 25  10   4]]\n",
      "0.01 32 40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95703, saving model to weights_best_5153projet.hdfs\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95703 to 0.88858, saving model to weights_best_5153projet.hdfs\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.88858\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.88858\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.88858\n",
      "[[65 85  9]\n",
      " [ 8 54  2]\n",
      " [ 4 17 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.50      0.60       199\n",
      "           1       0.35      0.70      0.46        80\n",
      "           2       0.38      0.23      0.29        39\n",
      "\n",
      "    accuracy                           0.52       318\n",
      "   macro avg       0.49      0.48      0.45       318\n",
      "weighted avg       0.60      0.52      0.53       318\n",
      "\n",
      "[[99 88 12]\n",
      " [21 56  3]\n",
      " [13 17  9]]\n"
     ]
    }
   ],
   "source": [
    "for j in [0,1,2,3,4]:\n",
    "    for l in [0.01]: #[0.1,0.05,0.01,0.005]:\n",
    "        for f in [32]:#[32,64]:\n",
    "            for ls in [40]:#[20, 40,80]:\n",
    "                print(l,f,ls)\n",
    "                model = Sequential()\n",
    "                model.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=200,weights=[embedding_matrix],\n",
    "                                            trainable=True))\n",
    "                model.add(Conv1D(filters=f, kernel_size=3, padding='same', activation='relu'))\n",
    "                model.add(MaxPooling1D(pool_size=2))\n",
    "                model.add(LSTM(ls))\n",
    "                \n",
    "                model.add(Dense(3, activation='softmax'))\n",
    "                                    \n",
    "                                              \n",
    "                optimizer = keras.optimizers.Adam(lr=l)\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                name1 = 'weights_best_5153projet.hdfs'\n",
    "                filepath= name1\n",
    "                checkpoint = ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, verbose=1,  mode='min',save_weights_only=True)\n",
    "                callbacks_list = [checkpoint]\n",
    "                model.fit(X_train, y_train_dummy, epochs=5,batch_size = 10, verbose = 0,workers=4, use_multiprocessing=True,callbacks = callbacks_list,validation_data=(X_val,y_test_dummy))\n",
    "                \n",
    "                from sklearn.metrics import confusion_matrix\n",
    "                pred1 = list()\n",
    "                y_pred1 = model.predict(X_val)\n",
    "                from sklearn.metrics import classification_report\n",
    "                \n",
    "                for i in range(len(y_pred1)):\n",
    "                    pred1.append(np.argmax(y_pred1[i]))\n",
    "                \n",
    "                print(confusion_matrix(val['Outcome'], pred1))\n",
    "                \n",
    "                pred2 = list()\n",
    "                y_pred2 = model.predict(X_test)\n",
    "                \n",
    "                    \n",
    "                for i in range(len(y_pred2)):\n",
    "                    pred2.append(np.argmax(y_pred2[i]))\n",
    "                    \n",
    "                print(classification_report(test['Outcome'], pred2, labels=[0,1,2]))\n",
    "                \n",
    "                \n",
    "                print(confusion_matrix(test['Outcome'], pred2))\n",
    "                \n",
    "                \n",
    "                from tensorflow.keras.backend import clear_session\n",
    "                clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
